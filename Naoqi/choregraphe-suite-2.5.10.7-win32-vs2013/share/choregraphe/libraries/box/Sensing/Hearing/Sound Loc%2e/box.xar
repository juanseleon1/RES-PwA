<?xml version="1.0" encoding="UTF-8" ?>
<ChoregrapheBox xar_version="3">
    <Box name="root" id="-1" tooltip="Every time a sound is detected, this box sends on its outputs:&#x0A;- the location of the sound source in relation to robot&apos;s head position.&#x0A;- robot&apos;s head position in relation to the rest of the body in the robot&apos;s space.&#x0A;&#x0A;V1.1.0" x="172" y="144">
        <bitmap>media/images/box/interaction/target_sound.png</bitmap>
        <script language="4">
            <content>
                <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.bIsRunning = False

    def onUnload(self):
        self.bIsRunning = False

    def onInput_onStart(self):
        self.bIsRunning = True
        try:
            proxy = ALProxy("ALSoundLocalization")
            proxy.setParameter( "Sensitivity", self.getParameter("Volume sensitivity (%)")/100. )
        except Exception as e:
            self.logger.error(e)

    def onInput_onStop(self):
        if( self.bIsRunning ):
            self.onUnload()
            self.onStopped()]]>
</content>
        </script>
        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
        <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
        <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
        <Input name="ALSoundLocalization/SoundLocated" type="0" type_size="1" nature="4" stm_value_name="ALSoundLocalization/SoundLocated" inner="1" tooltip="Connected to ALMemory. Will be stimulated every time the value subscribed to changes, respecting the refresh period." id="4" />
        <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is stopped." id="5" />
        <Output name="sourceLocation" type="2" type_size="2" nature="2" inner="0" tooltip="Location of the detected sound source including two angles (in radians):&#x0A;- azimutal (horizontal) angle&#x0A;- elevation (vertical) angle&#x0A;Both related to robot&apos;s head position." id="6" />
        <Output name="headPosition" type="2" type_size="6" nature="2" inner="0" tooltip="Head position when the sound is detected. It contains six values:&#x0A;[x,y,z,wx,wy,wz]. The first three represents the translation of the head&#x0A;related to the robot&apos;s space. The last three represents the orientation of the head.&#x0A;&#x0A;Note: If module ALMotion is not loaded, this output returns -1.&#x0A; " id="7" />
        <Parameter name="Threshold to be sure of the location (%)" inherits_from_parent="0" content_type="1" value="50" default_value="50" min="0" max="100" tooltip="If a sound is localized with a threshold higher than this value, then the sound&#x0A;location will be sent on the output. Else, the robot will consider that the sound is not&#x0A;localized at the right location and he will not take it into account." id="8" />
        <Parameter name="Volume sensitivity (%)" inherits_from_parent="0" content_type="1" value="90" default_value="90" min="0" max="100" tooltip="Sensitivity to the volume of the sound heard. Higher this parameter is, less the&#x0A;sound have to be loud to be detected." id="9" />
        <Timeline enable="0">
            <BehaviorLayer name="behavior_layer1">
                <BehaviorKeyframe name="keyframe1" index="1">
                    <Diagram>
                        <Box name="ProcessSoundLoc" id="1" localization="8" tooltip="Process the sound localization extractor to filter and give the most interesting data." x="147" y="164">
                            <bitmap>media/images/box/interaction/target_sound.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        pass

    def onUnload(self):
        pass

    def onInput_onStart(self, p):
        if p[1][2] >= self.getParameter("Threshold to be sure of the location (%)")/100.:
            self.onLocation( [ p[1][0],p[1][1] ] )
            self.onHeadPosition( [ p[2][0] , p[2][1], p[2][2], p[2][3], p[2][4], p[2][5] ] )]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onStart" type="0" type_size="1" nature="1" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                            <Output name="onLocation" type="2" type_size="2" nature="2" inner="0" tooltip="Location of the detected sound source including two angles (in radians):&#x0A;- azimutal (horizontal) angle&#x0A;- elevation (vertical) angle&#x0A;Both related to robot&apos;s head position." id="3" />
                            <Output name="onHeadPosition" type="2" type_size="6" nature="2" inner="0" tooltip="Head position when the sound is detected. It contains six values:&#x0A;[x,y,z,wx,wy,wz]. The first three represents the translation of the head&#x0A;related to the robot&apos;s space. The last three represents the orientation of the head.&#x0A;&#x0A;Note: If module ALMotion is not loaded, this output returns -1." id="4" />
                        </Box>
                        <Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="4" />
                        <Link inputowner="0" indexofinput="7" outputowner="1" indexofoutput="4" />
                        <Link inputowner="0" indexofinput="6" outputowner="1" indexofoutput="3" />
                    </Diagram>
                </BehaviorKeyframe>
            </BehaviorLayer>
        </Timeline>
    </Box>
</ChoregrapheBox>
